# AI Prowler â€” Personal AI Knowledge Base

**Complete User Guide Â· Version 2.0**

**Ask questions about YOUR documents using AI â€” locally or via the cloud**

Local-first &nbsp;â€¢&nbsp; Optional cloud AI &nbsp;â€¢&nbsp; Complete privacy &nbsp;â€¢&nbsp; No subscription required

---

## ğŸ“¦ What's in the Box

```
AI-Prowler/
â”œâ”€â”€ RAG_RUN.bat              â† Double-click to launch
â”œâ”€â”€ INSTALL.bat              â† One-click installer (run once)
â”œâ”€â”€ UNINSTALL.bat            â† Clean removal tool
â”œâ”€â”€ rag_gui.py               â† GUI application
â”œâ”€â”€ rag_preprocessor.py      â† Core indexing & query engine
â”œâ”€â”€ create_shortcut.py       â† Desktop shortcut creator
â”œâ”€â”€ requirements.txt         â† Python package list (11 packages)
â”œâ”€â”€ rag_icon.ico             â† Application icon
â”œâ”€â”€ COMPLETE_USER_GUIDE.md   â† This guide
â””â”€â”€ generate_license.py      â† License tool (optional)
```

---

## âš¡ Quick Start

```
Step 1 â€” Double-click INSTALL.bat
  â€¢ Installs Python 3.11 if not found (automatic)
  â€¢ Installs all 11 required Python packages
  â€¢ Downloads and installs Ollama AI engine
  â€¢ Downloads default AI model â€” llama3.2:1b (~1.3 GB)
  â€¢ Downloads Whisper speech model â€” large-v3-turbo (~1.6 GB)
  â€¢ Creates "AI Prowler" shortcut on your Desktop
  â€¢ Total download: ~4 GB Â· Time: 15â€“30 minutes

Step 2 â€” Wait for "INSTALLATION COMPLETE"

Step 3 â€” Double-click the "AI Prowler" Desktop icon
  â€¢ GUI opens â€” no terminal window visible
  â€¢ AI model begins loading silently in background
  â€¢ Ready to index and query immediately
```

---

## ğŸ¯ What Is AI Prowler?

AI Prowler uses **RAG (Retrieval-Augmented Generation)** â€” when you ask a question it first searches your own indexed documents for relevant passages, then feeds those passages to an AI model that writes a grounded, accurate answer.

**What it does:**
- ğŸ“š Indexes documents, code, email, spreadsheets, and 55+ file types
- ğŸ” Answers questions using your own content, not just general knowledge
- ğŸ¤– Runs 100% offline using local Ollama (default)
- â˜ï¸ Optionally uses cloud AI â€” ChatGPT, Claude, Gemini, Grok, Llama API, Mistral Large
- ğŸ”’ Local-first â€” no cloud contact unless you explicitly add an API key
- ğŸ“¬ Deep email support â€” Gmail, Apple Mail, Thunderbird, Yahoo, and more
- âš¡ Incremental updates â€” only re-processes files that changed
- ğŸ¤ Voice input â€” speak questions via local Whisper speech recognition
- ğŸ“ File attachments â€” attach images and files to questions (images sent to cloud providers)
- ğŸ’¾ File Output Mode â€” AI-written code files get one-click Save buttons automatically
- â° Scheduled auto-updates â€” keep the index current automatically
- ğŸ’¡ 20+ AI models â€” local and cloud, tune speed vs. quality for your needs
- ğŸŸ¢ Auto-start Ollama â€” optionally launch the AI server automatically

**Example conversation:**
```
You:         "What was the mutation rate in my NEAT project?"

AI Prowler:  According to NEAT_Documentation.md, the mutation rate
             is set to 0.02 (2%). This controls how frequently
             weights and connections mutate during evolution...
```

---

## ğŸš€ Launching AI Prowler

| Method | How |
|--------|-----|
| Desktop icon (easiest) | Double-click "AI Prowler" |
| From install folder | Double-click `RAG_RUN.bat` |
| Command line | `python rag_gui.py` |

When the GUI opens, the embedding model begins warming up in the background. If **Auto-start Ollama** is enabled in Settings, the Ollama server also launches automatically â€” you do not need to start it manually.

---

## ğŸ›ï¸ Menu Bar

### File
- **Exit** â€” close AI Prowler

### Help
- **ğŸ“– User Guide** â€” opens this guide in a built-in scrollable viewer window
- **ğŸš€ Quick Start** â€” abbreviated quick-start guide in a separate window
- **â„¹ï¸ About AI Prowler** â€” version, feature list, and credits

> **Note:** The User Guide viewer loads this `COMPLETE_USER_GUIDE.md` file directly from the installation folder. Keep the file in the same directory as `rag_gui.py` for the best reading experience.

---

## ğŸ“š Tab 1 â€” Index Documents

**Purpose:** Add documents to your knowledge base.

### The Directory Queue

AI Prowler uses a **queue system** â€” stage as many folders and individual files as you want, then process them all in one batch.

**Adding items to the queue:**

| Button | What it does |
|--------|-------------|
| ğŸ“‚ Browseâ€¦ â–¼ | Opens a dropdown menu with two choices â€” see below |
| ğŸ“„ Browse Files (multi-select)â€¦ | Standard file picker; Ctrl/Shift click to select multiple files |
| ğŸ“ Browse Folderâ€¦ | Opens a folder browser for selecting a single directory |
| â• Add to Queue | Adds whatever is typed in the path entry box |
| Type + Enter | Type a path directly and press Enter |

The **Queue counter** at the top right updates live as you add and remove items.

**Managing the queue:**

| Button | What it does |
|--------|-------------|
| âŒ Remove Selected | Removes the highlighted item before starting |
| ğŸ—‘ Clear Queue | Removes everything and starts fresh |
| Include subdirectories | Checkbox (default ON) â€” when checked every subfolder is scanned recursively |

### Options

**Smart scan** (default ON, recommended) â€” before indexing, AI Prowler pre-scans the queue and automatically skips:
- Executable and compiled binary files (`.exe`, `.dll`, `.pyc`, â€¦)
- Media files â€” images, audio, video, fonts
- Archive files â€” `.zip`, `.rar`, `.7z`, â€¦
- Database and VM image files
- Known system/tool directories â€” `.git`, `node_modules`, `__pycache__`, `venv`, `build`, `dist`, `.idea`, `.vscode`, and more

**Pre-scan only** â€” check this to see a full report of what *would* be indexed without actually indexing anything. Useful before committing to a large folder.

### Action Buttons

| Button | Function |
|--------|---------|
| â–¶ Start Indexing Queue | Begin processing all queued items |
| â¸ Pause | Freeze at the end of the current file. Click again to Resume |
| â¹ Stop | Stop cleanly after the current file and save position |
| ğŸ” Scan Queue | Run pre-scan and show report without indexing |

### Progress Display

Three live indicators appear while indexing runs:

- **Animated progress bar** on the left
- **Directory/file counter** in the centre â€” e.g. `Dir 2/4: Projects` or `[Email 847/12,034] Re: Budget`
- **Elapsed timer** on the right â€” ticks up in real time (e.g. `â± 3m 42s`)

For email archives the counter shows per-message progress so you always know exactly where you are inside a large export.

### Pause and Resume

- **Pause** freezes the worker thread immediately. The timer pauses too. Click **Resume** to continue from exactly where you stopped.
- **Stop** saves your exact position â€” the Start button changes to **â–¶ Resume Indexing**. Clicking it picks up from where you stopped, including the position within a partially-processed directory.

### What Gets Indexed

**Documents:** `.txt` `.md` `.rst` `.rtf` `.odt` `.pdf` `.docx` `.doc` `.xlsx` `.xls` `.pptx` `.ppt`

**Code and markup:** `.py` `.js` `.ts` `.jsx` `.tsx` `.cs` `.java` `.cpp` `.c` `.h` `.hpp` `.go` `.rs` `.rb` `.php` `.swift` `.kt` `.scala` `.r` `.html` `.htm` `.css` `.scss` `.sass` `.less` `.xml` `.xhtml`

**Config and data:** `.json` `.yaml` `.yml` `.toml` `.ini` `.cfg` `.conf` `.env` `.csv` `.tsv` `.log` `.sql`

**Scripts:** `.sh` `.bash` `.zsh` `.ps1` `.bat` `.cmd` `.gitignore` `.dockerignore` `.editorconfig`

**Email â€” single-message files:**
`.eml` `.msg` `.emlx`

**Email â€” multi-message archives** *(incremental indexer â€” see Email chapter below):*
`.mbox` `.rmail` `.babyl` `.mmdf`

### After Indexing

When a directory finishes indexing it is automatically **registered for tracking** â€” it appears in the Update Index tab and becomes eligible for scheduled updates.

---

## ğŸ“¬ Email Indexing â€” Complete Guide

AI Prowler has first-class support for email from every major provider. This chapter covers how the engine works and exactly how to export from each service.

---

### How It Works

**Single-message files** (`.eml`, `.msg`, `.emlx`) are indexed like any other document â€” one file in, one record out. The standard file-change tracker handles re-indexing: if the file's modification time hasn't changed since the last run, it is skipped.

**Multi-message archives** (`.mbox`, `.rmail`, `.babyl`, `.mmdf`) use a completely different engine called the **per-email incremental indexer**:

1. Every message in the archive is identified by a **stable unique ID** derived from its `Message-ID` header â€” an RFC 5322 globally-unique string assigned by the sending mail server. When a message has no `Message-ID`, a fingerprint is computed from `From + Date + Subject` instead.

2. A local database at `~/.rag_email_index.json` records which IDs have already been indexed for each archive file path.

3. On every re-import run, the engine compares the set of IDs in the archive against the set already in the database:
   - **New ID** (in archive, not in database) â†’ message is indexed and its chunks are added to ChromaDB
   - **Known ID** (in both) â†’ message is skipped entirely â€” no re-processing
   - **Removed ID** (in database but no longer in archive) â†’ its chunks are automatically deleted from ChromaDB

4. A 100,000-message archive that gained 200 new emails this week processes only those 200 â€” not the whole archive.

**Stop response:** The Stop button is checked after every single message (not just between files), so clicking Stop while processing a 50 GB archive responds within a second or two.

**Per-message progress:** The output panel shows `[Email 4,271/52,000] Re: Q3 Budget (87 words)` so you always know what is happening and can estimate completion time.

---

### Supported Archive Formats

| Format | Extension(s) | Notes |
|--------|-------------|-------|
| Unix mbox | `.mbox` | The most common export format. Used by Gmail Takeout, Thunderbird, Apple Mail, iCloud Mail, and many others |
| GNU Babyl / RMAIL | `.rmail` `.babyl` | GNU Emacs mail format â€” rare but fully supported |
| MMDF | `.mmdf` | Legacy SCO/Unix mail server format â€” rare but fully supported |

Single-message formats `.eml`, `.msg`, and `.emlx` are supported natively â€” no special configuration needed, just add the files or their containing folder to the index queue.

---

### Exporting From Every Major Provider

---

#### Gmail (Google)

Gmail exports in `.mbox` format â€” one file per label â€” via Google Takeout.

**Steps:**
1. Go to [takeout.google.com](https://takeout.google.com) and sign in
2. Click **Deselect all**, then scroll down and check only **Mail**
3. Click **All Mail data included** to choose specific labels (Inbox, Sent, a project label, etc.) rather than your entire mailbox if you don't need everything
4. Choose delivery: `.zip`, frequency: **Export once**, size: up to 50 GB per file
5. Click **Create export** â€” Google emails a download link when it's ready (minutes to hours depending on mailbox size)
6. Download and extract the `.zip` â€” inside you will find files named like `All mail Including Spam and Trash.mbox` or one `.mbox` per label
7. Add the `.mbox` file(s) to the AI Prowler index queue

> **Tip:** Label-by-label exports are easier to manage. Export just "Work" or "Projects" if that is all you need to query.

> **Re-exporting:** When you export again next month, Google regenerates the `.mbox` from scratch with all messages including new ones. AI Prowler's incremental indexer handles this correctly â€” it uses `Message-ID` to identify what's new, so only genuinely new messages are processed even though the whole file is new.

---

#### Apple Mail and iCloud Mail

Apple Mail stores mail internally as `.emlx` files and can export entire mailboxes as `.mbox` bundles.

**Export as .mbox (recommended for large mailboxes):**
1. Open the Mail app on your Mac
2. In the sidebar, select the mailbox you want to export (e.g. Inbox, a project folder)
3. Go to **Mailbox â†’ Export Mailboxâ€¦**
4. Choose a save location and click **Choose**
5. Apple Mail saves a `.mbox` package â€” on macOS this looks like a folder but on Windows (after copying) it is treated as a standard `.mbox` file
6. Add it to the AI Prowler index queue

**Access raw .emlx files directly (no export needed):**
If you have access to the macOS filesystem, Apple Mail's internal storage is at `~/Library/Mail/`. Each message is an individual `.emlx` file. Add the `Mail` folder or specific account sub-folders to the AI Prowler queue â€” smart scan will find and index all `.emlx` files recursively.

**iCloud Mail** uses the same Apple Mail client, so the export process is identical. Make sure your iCloud Mail is synced to the local Mail app first (Mail â†’ Preferences â†’ Accounts â†’ check the account is enabled and synced).

---

#### Thunderbird (Mozilla)

Thunderbird stores each folder as a single raw `.mbox` file on disk â€” **no export step is needed**. You point AI Prowler directly at the profile folder.

**Finding your Thunderbird mbox files:**

| OS | Default path |
|----|-------------|
| Windows | `C:\Users\YourName\AppData\Roaming\Thunderbird\Profiles\[profile]\Mail\` |
| macOS | `~/Library/Thunderbird/Profiles/[profile]/Mail/` |
| Linux | `~/.thunderbird/[profile]/Mail/` |

Inside each account folder you will find files named `Inbox`, `Sent`, `Drafts`, etc. with no file extension â€” these are standard mbox files. You can either:

- Add the entire `Mail` folder to the AI Prowler queue. Smart scan will find all mbox files automatically.
- Copy specific mailbox files, rename them with a `.mbox` extension, and add those instead.

> **Keeping it current:** Because Thunderbird's mbox files live on your disk permanently and are updated as new mail arrives, you can schedule AI Prowler to re-scan the Thunderbird folder weekly. The incremental indexer will pick up only new messages each time.

---

#### Yahoo Mail

Yahoo does not provide a direct export tool. The recommended path is to use a third-party tool to pull your mail via IMAP and save it as `.mbox`.

**Recommended approach â€” Thunderbird bridge:**
1. Add your Yahoo account to Thunderbird using IMAP
2. Let Thunderbird sync (can take hours for a large mailbox)
3. Point AI Prowler at the Thunderbird profile folder as described above

**Yahoo IMAP settings for Thunderbird:**
- Server: `imap.mail.yahoo.com` Â· Port: `993` Â· SSL/TLS: Yes
- You **must** use a Yahoo App Password â€” go to [security.yahoo.com](https://security.yahoo.com) â†’ Manage app passwords â†’ Generate one for Thunderbird. Your regular Yahoo password will not work for IMAP.

**Alternative â€” MailStore Home (free):**
1. Download MailStore Home from [mailstore.com/en/products/mailstore-home](https://www.mailstore.com/en/products/mailstore-home)
2. Add Yahoo as a source using the IMAP settings above
3. Export to `.mbox` format
4. Add the exported file to the AI Prowler queue

---

#### Outlook / Microsoft 365 / Exchange

Outlook's native format is `.pst`/`.ost` â€” a proprietary binary format that requires conversion. The cleanest approach depends on how many emails you need.

**Option A â€” Drag to folder (small batches):**
1. Open Outlook
2. Select messages (Ctrl+A to select all in a folder)
3. Drag and drop them onto a Windows folder â€” Outlook saves each as an `.eml` file
4. Add that folder to the AI Prowler index queue

**Option B â€” MailStore Home (large mailboxes, recommended):**
1. Download MailStore Home (free) from [mailstore.com](https://www.mailstore.com/en/products/mailstore-home)
2. Add your Outlook/Exchange account or import from a `.pst` file
3. Export to `.mbox` format
4. Add the `.mbox` to the AI Prowler queue

**Option C â€” Aid4Mail or similar PST converter:**
Converts `.pst` directly to `.mbox`. Several free and paid tools are available â€” search for "PST to mbox converter".

> **Note:** `.pst` and `.ost` files cannot be indexed directly because they use a proprietary binary format that requires Microsoft libraries to read. Conversion to `.mbox` or a folder of `.eml` files first is the reliable path.

---

#### Windows Live Mail / Windows Mail (legacy)

These apps stored each message as an individual `.eml` file in a folder hierarchy on disk.

**Default storage location:**
`C:\Users\YourName\AppData\Local\Microsoft\Windows Live Mail\`

Add that folder (or specific account sub-folders) directly to the AI Prowler index queue â€” smart scan finds all `.eml` files recursively.

---

#### Other Clients

| Client | How to export |
|--------|--------------|
| **Evolution** (Linux) | File â†’ Save As Mbox |
| **KMail** (Linux) | Folder â†’ Export â†’ mbox |
| **Mutt / Neomutt** | Uses mbox or Maildir natively â€” add the mbox file or folder directly |
| **Postfix / Dovecot** | Maildir format â€” add the mail spool directory |
| **Proton Mail** | Use Proton Mail Bridge (IMAP) â†’ Thunderbird â†’ AI Prowler |
| **Fastmail** | Settings â†’ Export â†’ mbox per folder |
| **Zoho Mail** | Settings â†’ Data Migration â†’ Export â†’ mbox |

---

### Re-Importing Updated Archives

When you export a fresh copy of your Gmail `.mbox` or Thunderbird folder next month:

- **New messages** added since the last import â†’ indexed
- **Messages present in both** old and new export â†’ skipped (already indexed)
- **Messages in old export but absent from new** â†’ chunks automatically removed from ChromaDB

This works because AI Prowler tracks individual `Message-ID` values, not file modification times. Even when Google Takeout regenerates the entire `.mbox` from scratch, only genuinely new messages are processed.

---

## ğŸ” Tab 2 â€” Ask Questions

**Purpose:** Ask natural language questions about your indexed documents, using either a local AI model or a cloud AI provider.

### Asking a Question

1. Click the **ğŸ” Ask Questions** tab
2. Type your question in the text box â€” or use the ğŸ¤ mic button (see below)
3. Optionally attach files using the **ğŸ“ Attachments** panel
4. Select your preferred AI provider from the **AI Provider** dropdown
5. Press **Ctrl+Enter** or click **Ask Question**

The model pre-warms automatically when you switch to this tab, so the first query is faster than it would otherwise be.

### Question Input Box

The question box accepts multi-line input. **Enter** adds a new line; **Ctrl+Enter** submits.

### Action Buttons

| Button | What it does |
|--------|-------------|
| **Ask Question** | Submits the question and begins the query |
| **â¹ Stop** | Cancels the current query in progress |
| **ğŸ’¾ Save Answer** | Saves the full answer text to a `.txt` or `.md` file |
| **âš¡ Load AI Model** | Manually triggers the Ollama model pre-warm â€” useful to get the model ready before you start typing |

### Model Status Indicator

A small **coloured dot** and status label appear to the right of the action buttons, showing the real-time state of the local Ollama model:

| Indicator | Meaning |
|-----------|---------|
| âš« Grey â€” "Model not loaded" | Ollama has not yet been contacted |
| ğŸŸ¡ Yellow â€” "Loading modelâ€¦" | Pre-warm is in progress; model is being loaded into memory |
| ğŸŸ¢ Green â€” "Model ready" | The model is loaded and queries respond quickly |

If you see grey and your first query feels slow, click **âš¡ Load AI Model** to pre-warm before you need it.

### ğŸ“ Attachments

The **Attachments** panel lets you add files to your question â€” useful for asking the AI to analyse, compare, or generate code based on existing files.

**To attach files:**
1. Click **ğŸ“ Attach Filesâ€¦** â€” a standard file picker opens supporting multi-select
2. Attached files appear as chips below the button showing an icon and filename
3. Click the **âœ•** on any chip to remove that file individually, or click **ğŸ—‘ Clear All** to remove everything

**Supported attachment types:**

- **Images** (`.png` `.jpg` `.jpeg` `.gif` `.bmp` `.webp` `.tiff`) â€” sent as base64 to cloud providers that support vision (ChatGPT, Claude, Gemini). Local Ollama also supports image input on compatible models.
- **Text files** (any other extension) â€” file content is read and included in the question prompt alongside the question text.

> **Cloud AI tip:** Image attachments require a cloud provider with vision support. If you are using Local Ollama with a text-only model, images in attachments will be ignored.

### ğŸ“„ File Output Mode

The **File Output Mode** checkbox (just below the Attachments panel) optimises AI answers when you ask the AI to write or modify code files.

**When ticked (default ON):** AI Prowler instructs the AI to label every code block it generates with an explicit filename. When the answer arrives, the app automatically scans for labelled code blocks and displays a **ğŸ“ Files in Answer** panel with a **ğŸ’¾ Save File** button for each detected file â€” no copy-pasting required.

**How the detection works:**

The engine looks for three patterns in the answer:

1. ` ```python my_script.py ` â€” language + filename (most common)
2. ` ```my_script.py ` â€” filename-only fence (no language prefix)
3. `### FILE: name.ext ###` â€¦ `### END FILE ###` â€” explicit block markers

The **ğŸ“ Files in Answer** panel shows each detected file's name, line count, and a **ğŸ’¾ Save File** button that opens a Save-As dialog pre-populated with the correct filename and file type filter.

**When unticked:** The AI answers normally without being prompted to label files. Useful for conversational questions where you do not want code outputs.

### AI Provider Selector

The **AI Provider** dropdown lets you choose which AI answers your question. It appears in the options row alongside the Context Chunks control.

A small **coloured status light** to the left of the dropdown shows the current provider's state at a glance:

| Light colour | Meaning |
|-------------|---------|
| âš« Grey | No API key configured, or local Ollama |
| ğŸŸ¢ Green | External provider ready â€” key present, not rate-limited |
| ğŸŸ  Orange | Provider is temporarily rate-limited |

Available providers (configure API keys in the Settings tab):

| Provider | Model used | Free tier |
|----------|-----------|-----------|
| **Local Ollama** (default) | Your selected local model | Free forever |
| **ChatGPT** (OpenAI) | GPT-4o | Pay-per-use |
| **Claude** (Anthropic) | claude-opus-4-5 | $5 free credit |
| **Gemini** (Google) | gemini-2.0-flash | âœ… Free tier |
| **Grok** (xAI) | grok-beta | Limited free |
| **Llama API** (Meta) | Llama-4-Scout-17B-16E-Instruct | âœ… Free tier |
| **Mistral Large** (Mistral AI) | mistral-large-latest | Limited free |

> **Auto-fallback:** If an external provider fails or hits its rate limit, AI Prowler automatically falls back to your local Ollama model and shows an error note in the answer. You can disable this in Settings â†’ External AI APIs.

### Context Chunks

The **Context chunks** dropdown controls how many document excerpts are retrieved from the index to give the AI context for its answer.

| Setting | Best for |
|---------|---------|
| Auto (3) | Calculates optimally for most questions â€” recommended |
| 1â€“5 | Quick factual lookups |
| 6 | Broader questions spanning multiple files |
| 7 âš reload â€“ 20 âš reload | Wide coverage / summarisation â€” **triggers model reload**, adds 2â€“12 minutes on CPU |

Values marked **âš reload** require a larger context window than the default model configuration. AI Prowler will automatically re-prewarm the model at the required size when you change to these values â€” a status message appears during the reload. On GPU systems this is much faster.

### Progress and Timing

A **progress bar** animates while the query runs. An **elapsed timer** ticks up in real time. When the answer arrives the timer freezes â€” e.g. `âœ… 14s`.

### Example Questions

```
Factual lookups:
  "What was the mutation rate in my NEAT config?"
  "What's the deadline for the Smith project?"
  "Find my flight confirmation number for the Paris trip"

Broad summaries:
  "What documents do I have about machine learning?"
  "Summarise my project documentation"
  "What are the recurring issues in my support tickets?"

Technical / coding:
  "Show me all Python functions that use asyncio"
  "What libraries are imported in my backend code?"
  "Write me a script to parse the CSV files in my data folder"

Email:
  "What did John say about the Q3 budget?"
  "Find any emails about the server outage in January"
  "What agreements did I make with Acme Corp last year?"

With attachments (cloud AI):
  "Here's my current login.py â€” add OAuth2 support"
  "What errors are in this screenshot?"
  "Refactor this code to follow PEP 8"
```

### Voice Input (ğŸ¤ Microphone)

When `faster-whisper`, `sounddevice`, and `numpy` are installed (they are by default), a microphone button and controls appear below the question box.

| State | What to do |
|-------|-----------|
| ğŸ¤ (grey) | Click to start recording |
| ğŸ”´ (red, recording) | Speak your question â€” click again to stop early |
| Transcribingâ€¦ | Whisper is converting speech to text |
| Question populated | Review, edit if needed, then press Ctrl+Enter |

**Append mode** â€” the **Append (add to existing text)** checkbox controls whether dictated text is added to whatever is already in the question box (ON) or replaces it (OFF).

**ğŸ—‘ Clear Question** â€” clears the question box and resets mic status.

**Auto-stop:** recording ends automatically after a configurable silence period (default 3 seconds). Adjust the threshold in Settings â†’ Microphone / Speech Input.

The Whisper `large-v3-turbo` model (~1.6 GB) is downloaded once on first use and cached. Subsequent launches load it instantly.

---

## ğŸ”„ Tab 3 â€” Update Index

**Purpose:** Keep your knowledge base current without re-indexing everything.

### How File Tracking Works

When a directory is indexed, AI Prowler records each file's path, modification time, and size in `~/.rag_file_tracking.json`. On the next update run:

| File status | What happens |
|-------------|-------------|
| New file | Indexed and added to ChromaDB |
| Modified file | Old chunks deleted, new chunks added |
| Deleted file | Chunks removed from ChromaDB |
| Unchanged file | Skipped entirely |

For email archives the engine goes deeper â€” see the Email chapter for how per-message deduplication works.

### Tracked Directories List

Shows every directory registered for tracking. The info bar at the top shows the exact paths of both tracking data files so you know where they live â€” they are **separate from the ChromaDB database** and survive a database wipe. Click **ğŸ”„ Refresh List** to reload from disk.

### Update Buttons

| Button | What it does |
|--------|-------------|
| Update Selected | Updates only the highlighted directory |
| Update All | Updates every tracked directory in sequence |

Both buttons run the full change detection pipeline and show a per-file log in the output panel.

### Removing a Tracked Directory

Select a directory and click **ğŸ—‘ Remove Selected (untrack + delete its vectors)**. This does four things atomically:
1. Removes the directory from the auto-update list
2. Deletes all file-tracking timestamps for that directory
3. Deletes all ChromaDB chunks whose filepath falls within that directory
4. Removes any email index entries for archive files inside that directory

The actual files on disk are NOT touched. You can re-index the directory later if needed.

---

## â° Tab 4 â€” Schedule

**Purpose:** Run automatic index updates on a timer using Windows Task Scheduler.

### Why Schedule

Your documents change constantly. Scheduling ensures the AI always knows your latest content without you having to remember to click Update.

### Quick Schedule Presets

| Preset | Runs |
|--------|------|
| Daily at 8:00 AM | Every day at 8 AM |
| Daily at 9:00 AM | Every day at 9 AM |
| Weekdays at 8:00 AM | Mondayâ€“Friday at 8 AM |

### Custom Schedule

1. Enter a time in **HH:MM** 24-hour format â€” e.g. `07:30`, `13:00`, `22:15`
2. Choose **DAILY** or **WEEKDAYS**
3. Click **Set Schedule**

### Schedule Controls

| Control | Effect |
|---------|--------|
| Disable Schedule | Suspends the task without deleting it |
| Remove Schedule | Permanently deletes the Task Scheduler task |
| Refresh Status | Polls Task Scheduler and refreshes the display |

### Status Display

```
Active:
  âœ… Schedule Active
  Next Run: 2/25/2026 8:00 AM

Not set:
  âŒ No Schedule Set
```

### Requirements

- At least one tracked directory in the Update Index tab
- Windows Task Scheduler service running (on by default in all Windows versions)
- AI Prowler installed in a permanent location â€” the task uses the full install path

---

## ğŸ—‚ Tab 5 â€” Auto Scan Config

**Purpose:** Control exactly which file types and directories are included or excluded during smart scan.

All changes take effect immediately and are saved to `~/.rag_config.json`. They apply to every future scan and update run.

### Supported Extensions (left panel)

The **âœ… Supported Extensions** list contains every file type that will be indexed. The default list covers 55+ types.

- **â• Add** â€” type an extension (e.g. `.nfo`) and press Enter or click Add. The leading dot is added automatically if you omit it.
- **âŒ Remove** â€” click an extension to select it, then click Remove.
- **Conflict detection** â€” if you try to add an extension that already exists in the Skipped list, AI Prowler warns you and blocks the add.

### Skipped Extensions (right panel)

The **ğŸš« Skipped Extensions** list contains types that are always ignored â€” compiled binaries, media, archives, etc. Same Add/Remove controls.

### Skipped Directories (bottom panel)

The **ğŸ“‚ Skipped Directories** list contains folder *names* (not full paths) that are skipped when walking any directory tree. Defaults include:

- Version control: `.git` `.svn` `.hg` `.bzr`
- Package managers: `node_modules` `vendor` `.nuget`
- Python: `__pycache__` `.venv` `venv` `site-packages`
- Build output: `build` `dist` `bin` `obj` `target`
- IDE folders: `.idea` `.vscode` `.vs`
- AI Prowler's own database: `rag_database`

Add project-specific folders (e.g. `backup`, `.cache`, `temp`) to exclude them from all future scans.

### Save and Reset

| Button | Effect |
|--------|--------|
| ğŸ’¾ Save Changes | Explicitly saves (changes also auto-save as you edit) |
| â†© Reset to Defaults | Restores all three lists to built-in defaults â€” asks for confirmation |

---

## âš™ï¸ Tab 6 â€” Settings

**Purpose:** Configure the AI model, external cloud AI providers, GPU acceleration, Ollama server behaviour, query output format, voice input, and database tools.

The Settings tab is scrollable â€” scroll down to see all sections.

---

### AI Model

**Select model** â€” choose from the full list of Ollama-compatible local models. The dropdown displays each model with its download size and minimum RAM requirement:

```
âœ… llama3.2:1b  [1.3 GB dl | 4 GB RAM]
âœ… llama3.2:3b  [2.0 GB dl | 6 GB RAM]
âœ… llama3.1:8b  [4.7 GB dl | 8 GB RAM]
âš ï¸ qwen2.5:14b  [9.0 GB dl | 16 GB RAM]
```

AI Prowler automatically detects your system RAM and adds a **fitness badge** to every model:

| Badge | Meaning |
|-------|---------|
| âœ… | Model fits in your RAM â€” recommended |
| âš ï¸ | Model needs more RAM than detected â€” may run slowly |

Models that fit in your RAM appear first in the list. A note below the dropdown confirms your detected RAM size.

**Browse & Install Modelâ€¦** â€” opens a full model browser where you can search, review, and download any Ollama-compatible model directly from within the app.

**Model families and trade-offs:**

| Family | Models | Best for |
|--------|--------|---------|
| Llama 3.2 | `llama3.2:1b` â­ `llama3.2:3b` | Default â€” fast and capable |
| Llama 3.1 | `llama3.1:8b` `70b` `405b` | High-quality answers |
| Llama 3 | `llama3:8b` `70b` | Proven quality (older generation) |
| Qwen 2.5 | `0.5b` through `72b` | Multilingual, ultra-lightweight options |
| Mistral | `mistral:7b` `mixtral:8x7b` `8x22b` | Code-heavy projects |
| Gemma | `gemma:2b` `7b` `gemma2:9b` `27b` | Google's models |

**Size vs. hardware guide:**

| Model size | Speed | Quality | Min RAM |
|-----------|-------|---------|---------|
| 0.5bâ€“1b | âš¡âš¡âš¡ | â­ | 4 GB |
| 3bâ€“7b | âš¡âš¡ | â­â­ | 8 GB |
| 8bâ€“14b | âš¡ | â­â­â­ | 16 GB |
| 70b+ | ğŸŒ | â­â­â­â­ | 32+ GB |

Start with `llama3.2:1b`. If answers feel shallow, upgrade to `llama3.2:3b` or `llama3.1:8b`.

---

### External AI APIs

This section lets you connect AI Prowler to cloud AI providers. Cloud providers typically give higher-quality answers for complex questions and support image attachments â€” at the cost of sending your prompts (but **not your raw documents**) to an external service.

> **Privacy note:** Only the question text and retrieved document excerpts are sent to cloud providers â€” not your original files. The RAG retrieval step always runs locally.

**Setting up a provider:**

1. Click **ğŸ”‘ Get Key** next to the provider â€” this opens the provider's API key page in your browser
2. Sign up / log in and generate an API key
3. Paste the key into the entry box in AI Prowler
4. Click **Save**
5. Optionally click **ğŸ”Œ Test** to verify the key works with a live ping

Each provider row contains:

| Element | Purpose |
|---------|---------|
| **Coloured status dot** | Shows provider status at a glance (see below) |
| **Provider name label** | e.g. "ChatGPT (OpenAI)" |
| **API key entry box** | Masked by default â€” paste your key here |
| **ğŸ‘ Toggle** | Show or hide the key characters |
| **Save** | Saves the key to `~/.rag_config.json` |
| **ğŸ”Œ Test** | Fires a live connection test and shows a result popup |
| **ğŸ”‘ Get Key (free note)** | Opens the provider's key page in your browser |

**Status dot colours:**

| Dot | Meaning |
|-----|---------|
| âš« Grey | No API key saved |
| ğŸŸ¢ Green | Key saved and connection verified |
| ğŸŸ  Orange | Provider is temporarily rate-limited |

**Provider reference:**

| Provider | Free tier note | Key URL |
|----------|---------------|---------|
| ChatGPT (OpenAI) | Pay-per-use | platform.openai.com/api-keys |
| Claude (Anthropic) | $5 free credit on sign-up | console.anthropic.com |
| Gemini (Google) | âœ… Generous free tier | aistudio.google.com |
| Grok (xAI) | Limited free | console.x.ai |
| Llama API (Meta) | âœ… Free tier available | llama.developer.meta.com |
| Mistral Large | Limited free | console.mistral.ai |

**Auto-fallback to Local Ollama** â€” when this checkbox is ON (default), if an external provider fails or returns a rate-limit error, AI Prowler silently retries with your local Ollama model and includes a brief error note in the answer. Uncheck to disable fallback and see the raw error instead.

---

### Database

| Button | Effect |
|--------|--------|
| View Statistics | Opens a dialog showing total chunks, unique files, and collection metadata |
| Clear Database | Permanently deletes all indexed content from ChromaDB â€” asks for confirmation. Does not affect the file-tracking database or email index. |

---

### Query Output

Controls what appears in the answer panel alongside the AI's response.

**Show source references** â€” when ON, the answer panel includes file paths, relevance scores, chunk counts, and query timing. When OFF (default), only the clean answer is shown.

**Enable debug output** â€” when ON, the answer panel includes detailed timing markers (â±), debug annotations (ğŸ”¬), and a DOS test command that shows the raw Ollama call. Useful for diagnosing slow or unexpected responses. When OFF (default), all debug lines are suppressed.

**Debug View** â€” when ON, any DOS/Command Prompt windows opened by AI Prowler (e.g. the Ollama server window) appear in the **foreground** on your desktop. When OFF (default), those windows open **silently in the background** â€” the server runs but no CMD window appears on screen. This setting affects the Ollama auto-start window and any subprocesses launched during queries.

> **Tip:** Use Debug View temporarily if you need to inspect Ollama server logs or troubleshoot connection issues, then turn it off for everyday use.

---

### Microphone / Speech Input

*(Only visible when faster-whisper, sounddevice, and numpy are installed)*

**Auto-stop after silence** â€” a slider from 1.0 to 8.0 seconds (in 0.5s steps) controlling how long Whisper waits after you stop speaking before ending the recording automatically.

- **Short (1â€“2s)** â€” snappy for short direct questions
- **Long (4â€“8s)** â€” better if you pause between phrases or speak slowly

The value is saved to config and persists across restarts. It also applies live to any recording already in progress.

---

### GPU Acceleration

Controls how many AI model layers Ollama offloads to your GPU. More layers on GPU means faster query responses on systems with a dedicated graphics card.

| Value | Meaning |
|-------|---------|
| -1 (default) | **Auto** â€” Ollama decides how many layers fit in available VRAM |
| 0 | **CPU only** â€” use if GPU causes errors or VRAM is insufficient |
| 1â€“99 | **Partial offload** â€” fine-tune for laptops with limited VRAM |

**ğŸ” Detect GPU** â€” runs a background scan that identifies your GPU model, VRAM size, and suggests an optimal layers value. The full detection output appears in a scrollable status box below the controls (long results are no longer cut off).

**âœ… Apply & Reload** â€” saves the layers value and reloads the Ollama configuration so it takes effect immediately on the next query â€” no app restart needed.

---

### Ollama Server

Controls how AI Prowler manages the Ollama backend process.

**Auto-start Ollama server (opens separate CMD window)**

When this checkbox is **enabled**:
- AI Prowler checks on startup whether Ollama is already running
- If Ollama is not running, it launches `ollama serve` automatically
- When AI Prowler is closed, the Ollama process is also shut down
- Whether the Ollama window is visible depends on the **Debug View** setting (see Query Output section above)

When this checkbox is **disabled** (default):
- AI Prowler does not start Ollama automatically
- You must start Ollama manually before using query features â€” open a Command Prompt and run `ollama serve`, or start it from the Windows Start menu

> **Recommendation:** Enable auto-start if you only use Ollama through AI Prowler and want a one-click experience. Leave it disabled if you run other Ollama-based tools and want the server to stay running independently of AI Prowler.

The setting is saved immediately and persists across restarts.

---

## ğŸ’» Command Line (Advanced)

All core functions are available without the GUI:

```bash
# Index a directory (recursive by default)
python rag_preprocessor.py index C:\Users\YourName\Documents

# Ask a question
python rag_preprocessor.py query "What is in my documents?"

# List indexed files
python rag_preprocessor.py list

# Show database statistics
python rag_preprocessor.py stats

# Scan a directory for changes without updating
python rag_preprocessor.py check C:\Users\YourName\Documents

# Update only changed files in a directory
python rag_preprocessor.py update C:\Users\YourName\Documents

# Auto-update all tracked directories
python rag_preprocessor.py auto-update

# Change the active AI model
python rag_preprocessor.py model llama3.1:8b

# Clear the entire database
python rag_preprocessor.py clear
```

---

## ğŸ”§ System Requirements

### Minimum

| Component | Requirement |
|-----------|------------|
| OS | Windows 10 or Windows 11 (64-bit) |
| RAM | 8 GB |
| Storage | 6 GB free |
| CPU | Any modern 64-bit processor |
| Internet | Required for installation only (and for cloud AI providers if used) |

### Recommended

| Component | Recommendation |
|-----------|---------------|
| RAM | 16 GB (enables 7bâ€“8b models) |
| Storage | 15 GB (room for multiple models) |
| CPU | Modern quad-core or better |
| GPU | Optional â€” significantly speeds up 7b+ models |

### Download Sizes (One-Time, Installation Only)

| Component | Size |
|-----------|------|
| Python 3.11 | ~30 MB |
| Python packages (11 total) | ~600 MB |
| Ollama engine | ~400 MB |
| AI model â€” llama3.2:1b (default) | ~1.3 GB |
| Whisper speech model â€” large-v3-turbo | ~1.6 GB |
| **Total** | **~4 GB** |

After installation: 100% offline by default, no cloud connections unless you configure cloud provider API keys.

---

## ğŸ“ File Locations

### Installation Folder

```
C:\Users\YourName\AI Prowler\
â”œâ”€â”€ rag_gui.py
â”œâ”€â”€ rag_preprocessor.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ RAG_RUN.bat
â”œâ”€â”€ INSTALL.bat
â”œâ”€â”€ UNINSTALL.bat
â”œâ”€â”€ create_shortcut.py
â”œâ”€â”€ rag_icon.ico
â”œâ”€â”€ COMPLETE_USER_GUIDE.md
â””â”€â”€ rag_database\               â† ChromaDB index (your indexed content)
    â””â”€â”€ [ChromaDB files]
```

### User Data Files (Home Folder)

```
C:\Users\YourName\
â”œâ”€â”€ .rag_config.json              â† All settings (model, GPU layers, silence, auto-start, API keys, etc.)
â”œâ”€â”€ .rag_auto_update_dirs.json    â† List of tracked directories
â”œâ”€â”€ .rag_file_tracking.json       â† File modification baselines for change detection
â”œâ”€â”€ .rag_email_index.json         â† Per-email Message-ID tracking for incremental indexing
â”œâ”€â”€ .rag_license.key              â† License key (if applicable)
â””â”€â”€ rag_auto_update.bat           â† Generated update script (created when you set a schedule)
```

> **API keys** are stored in `.rag_config.json` under your user home folder â€” not in the AI Prowler installation folder and not transmitted anywhere other than to the provider you explicitly selected.

### Speech Model Cache

```
C:\Users\YourName\.cache\huggingface\hub\
â””â”€â”€ models--Systran--faster-whisper-large-v3-turbo\   â† ~1.6 GB
    (Only this sub-folder is touched by AI Prowler)
```

---

## ğŸ” Privacy and Security

**AI Prowler is local-first. Cloud AI is entirely opt-in.**

| What it does | What it does NOT do |
|--------------|-------------------|
| âœ… Runs 100% locally by default | âŒ No cloud contact unless you add an API key |
| âœ… All local AI inference via Ollama | âŒ No automatic data uploads |
| âœ… No login or account required for local use | âŒ No telemetry or analytics |
| âœ… All indexed data stays on your hard drive | âŒ No phone-home behaviour |
| âœ… API keys stored locally in your home folder | âŒ Raw document files never sent to cloud |

**When cloud AI providers are used:**
- Only your question text and retrieved document **excerpts** (not full files) are sent to the provider
- The RAG retrieval step â€” finding relevant passages from your index â€” always runs locally
- Your original source files are never transmitted
- You choose when to use a cloud provider for each individual question via the AI Provider dropdown

---

## ğŸ—‘ï¸ Uninstalling

### Option 1 â€” UNINSTALL.bat (recommended)

Double-click `UNINSTALL.bat`. It walks through 9 clearly labelled steps:

| Step | What is removed |
|------|----------------|
| 1/9 | Windows Task Scheduler task |
| 2/9 | Desktop shortcut (both "AI Prowler.lnk" and "RAG.lnk" if present) |
| 3/9 | AI Prowler entry from Windows PATH (uses PowerShell for reliability) |
| 4/9 | Config files (`.rag_config.json`, `.rag_file_tracking.json`, `.rag_email_index.json`, etc.) |
| 5/9 | ChromaDB database (optional â€” confirms before deleting) |
| 6/9 | AI Prowler program files |
| 7/9 | Ollama engine |
| 8/9 | Whisper model cache â€” targets **only** the AI Prowler model folder, leaving other HuggingFace models untouched |
| 9/9 | Summary |

Python packages are intentionally kept to avoid breaking other programs. Remove Python separately via Settings â†’ Apps if needed.

### Option 2 â€” Manual Removal

Delete:
- The AI Prowler installation folder
- From your home folder: `.rag_config.json`, `.rag_auto_update_dirs.json`, `.rag_file_tracking.json`, `.rag_email_index.json`, `.rag_license.key`, `rag_auto_update.bat`
- Desktop shortcut (`AI Prowler.lnk`)
- Task Scheduler task â€” open Task Scheduler from Start menu, find and delete the "RAG Auto-Update" task
- Whisper cache: `C:\Users\YourName\.cache\huggingface\hub\models--Systran--faster-whisper-large-v3-turbo\`

---

## ğŸš¨ Troubleshooting

### Installation

| Problem | Solution |
|---------|---------|
| "Python not found" | Re-run INSTALL.bat â€” it installs Python 3.11 automatically |
| Package install failed | Check internet connection, re-run INSTALL.bat |
| "Ollama not found" | Download from [ollama.com/download/windows](https://ollama.com/download/windows) or re-run INSTALL.bat |
| Whisper download failed | Non-critical â€” model downloads on first mic button use |

### GUI

| Problem | Solution |
|---------|---------|
| GUI won't open | Run `python rag_gui.py` from Command Prompt to see the error message |
| "Could not import AI Prowler modules" | Ensure `rag_preprocessor.py` is in the same folder as `rag_gui.py` |
| Microphone button missing | Run `pip install faster-whisper sounddevice numpy`, then restart |
| Tab appears blank | Try launching via `RAG_RUN.bat` instead |
| Status indicator stays grey | Click **âš¡ Load AI Model** or check that Ollama is running |
| Settings checkbox not sticking | Verify you have write access to your home folder (`C:\Users\YourName\`) |

### Queries

| Problem | Solution |
|---------|---------|
| First query takes 2â€“3 minutes | Normal â€” the AI model is loading into memory for the first time. Use âš¡ Load AI Model beforehand to pre-warm |
| "Cannot connect to Ollama" | Enable Auto-start Ollama in Settings, or open Command Prompt and run `ollama serve` |
| Answers are vague | Try a larger model (Settings tab) or increase Context Chunks |
| "No results" | Make sure the relevant documents have been indexed |
| Context chunks âš reload is very slow | This is expected on CPU-only systems for >6 chunks â€” use a GPU or limit chunks to 5 or fewer |

### Cloud AI Providers

| Problem | Solution |
|---------|---------|
| ğŸ”Œ Test shows "Invalid API key" | Double-check the key was copied fully with no spaces; regenerate if needed |
| Provider returns HTTP 429 | Rate limit reached â€” AI Prowler shows a timeout until the quota resets, then resumes normally |
| Image attachments not working | Confirm you are using a cloud provider that supports vision (ChatGPT, Claude, Gemini) |
| Cloud answer but no document context | The RAG retrieval still runs locally â€” if it finds nothing, index the relevant documents first |
| Auto-fallback kicked in | The selected provider failed; answer came from local Ollama. Check the provider status dot in Settings |

### Email Indexing

| Problem | Solution |
|---------|---------|
| Large .mbox import is slow | Normal for first import â€” per-message progress shows in the output panel. Use Stop/Resume to spread across multiple sessions |
| Stop button is slow to respond | Stop now responds after each individual message â€” if delayed, the current message is still being processed |
| Re-importing same .mbox re-indexes everything | Incremental indexing uses Message-ID tracking to skip known messages. If re-indexing still occurs, check that the archive file path hasn't changed |
| Yahoo/Outlook won't import | These formats need conversion first â€” see the Email chapter above |
| Apple Mail .mbox looks like a folder | On macOS it is a package. Copy it to Windows, it becomes a regular `.mbox` file |

### Ollama Server

| Problem | Solution |
|---------|---------|
| No CMD window on startup | Expected when Debug View is OFF â€” Ollama runs silently in background. Enable Debug View in Settings â†’ Query Output to see the window |
| Ollama CMD window closed by accident | Re-enable auto-start and restart AI Prowler, or run `ollama serve` manually |
| Auto-start isn't launching Ollama | Ensure `ollama` is in your PATH â€” re-run INSTALL.bat or install Ollama from ollama.com/download/windows |

### Scheduling

| Problem | Solution |
|---------|---------|
| Schedule not running | Check the Schedule tab shows "âœ… Schedule Active" and verify Windows Task Scheduler is running |
| Can't create schedule | Run AI Prowler as Administrator (right-click â†’ Run as administrator) |
| Schedule shows wrong time | Remove and recreate; check Windows time zone settings |

---

## ğŸ“ Tips and Best Practices

### Indexing

âœ… Use Pre-scan first on any unfamiliar large folder  
âœ… Start with one focused project folder to test, then expand  
âœ… Use Pause/Stop freely â€” progress is always saved and resumable  
âœ… For email, keep exported archives in a dedicated folder and re-export periodically  

âŒ Don't index your entire C:\ drive  
âŒ Don't index temp folders, Downloads, or the Recycle Bin  
âŒ Don't run indexing and querying at the same time  

### Queries

âœ… Use complete natural-language questions  
âœ… Reference document names or dates when you know them  
âœ… Keep Context Chunks at Auto (3) or 3â€“5 for everyday use  
âœ… Only increase to âš reload chunks when you need broad coverage â€” be prepared for a wait on CPU  
âœ… Use voice input for longer or more natural questions  
âœ… Click âš¡ Load AI Model when you open AI Prowler to pre-warm while you work  
âœ… Enable File Output Mode when asking the AI to write or modify code â€” Save buttons appear automatically  

âŒ Don't use single keywords â€” the AI needs full context  
âŒ Don't ask about content that hasn't been indexed  

### Cloud AI Providers

âœ… Try Gemini or Llama API first â€” both have free tiers and are easy to set up  
âœ… Use cloud providers for complex, multi-document questions that need higher reasoning quality  
âœ… Use image attachments with ChatGPT, Claude, or Gemini for screenshot analysis or diagram understanding  
âœ… Keep Auto-fallback ON so queries always get an answer even if a provider is temporarily unavailable  

âŒ Don't put API keys anywhere other than the Settings â†’ External AI APIs fields  
âŒ Don't send highly sensitive personal data via cloud providers â€” use Local Ollama for maximum privacy  

### Email

âœ… Export by label/folder from Gmail rather than "All Mail" if you only need specific content  
âœ… Keep archive files at a stable path â€” the incremental indexer deduplicates by path + Message-ID  
âœ… Schedule weekly re-imports for actively-used mailboxes  
âœ… Use the per-message progress counter to estimate time for very large archives  

âŒ Don't delete and recreate archive files unnecessarily â€” the incremental engine works best when the file path stays the same  

### Performance

âœ… Use GPU layers = -1 (Auto) â€” Ollama optimises automatically  
âœ… Schedule updates during off-hours (overnight, lunch)  
âœ… Stick with `llama3.2:1b` unless you need higher answer quality  
âœ… Enable Auto-start Ollama for a seamless one-click launch experience  
âœ… Use Debug View only when troubleshooting â€” keep it OFF for everyday use  

âŒ Don't use 70b+ models unless you have 32+ GB RAM  
âŒ Don't run multiple AI Prowler instances simultaneously  

---

## â“ Frequently Asked Questions

**Q: Do I need an API key or account?**  
A: No â€” everything runs locally with no accounts, keys, or registration. Cloud AI providers are entirely optional and only used when you explicitly add a key.

**Q: Does this work offline?**  
A: Yes â€” 100% offline by default. Cloud providers obviously need an internet connection, but local Ollama queries work with no network at all.

**Q: Is my data private?**  
A: Completely private when using local Ollama. When you opt in to a cloud provider, only your question and retrieved excerpts are sent â€” your original files never leave your computer.

**Q: How much does it cost?**  
A: The app is free. Local Ollama is free. Cloud providers are billed by the provider â€” several offer generous free tiers (Gemini and Llama API in particular).

**Q: Does it need a GPU?**  
A: No. The default model runs well on CPU-only hardware. A GPU speeds up larger models significantly and makes high âš reload chunk counts much faster.

**Q: How many documents can I index?**  
A: Thousands â€” limited only by available disk space and ChromaDB index capacity.

**Q: My Gmail export is 8 GB. Will AI Prowler handle it?**  
A: Yes. The incremental indexer processes messages one at a time with Stop/Resume support, so you can spread a large initial import over multiple sessions. Future re-imports only process new messages.

**Q: Can I query email from 10 years ago?**  
A: Yes, as long as those emails are in the exported archive and have been indexed.

**Q: Do I need to re-index everything when files change?**  
A: No â€” the Update Index tab re-indexes only new and changed files. For email archives, only new messages are processed.

**Q: What if my computer is off when a schedule is due?**  
A: Windows Task Scheduler runs the task the next time the computer is on and the trigger time is reached.

**Q: Can I use a different AI model?**  
A: Yes â€” any Ollama-compatible model works. Install it from the Settings tab or by running `ollama pull <model-name>`.

**Q: What does the Auto-start Ollama option do?**  
A: When enabled, AI Prowler automatically launches the Ollama server when you open the app and shuts it down on exit. The server window is hidden by default â€” enable Debug View in Settings if you need to see it.

**Q: What is the âš¡ Load AI Model button for?**  
A: It manually triggers the model pre-warm so the AI is ready before you type your first question. The model loads automatically when you switch to the Ask Questions tab, but clicking this button lets you start loading while you're still on another tab.

**Q: What does File Output Mode do?**  
A: It instructs the AI to label any code or script files it writes with a filename. AI Prowler then detects those filenames in the answer and shows a ğŸ’¾ Save File button for each one â€” eliminating copy-paste for code file answers.

**Q: Can I attach images to questions?**  
A: Yes â€” use the ğŸ“ Attach Files button. Images are supported by cloud providers with vision capability (ChatGPT, Claude, Gemini). Text files can be attached regardless of provider.

**Q: What context chunks setting should I use?**  
A: "Auto (3)" is the best default â€” it calculates the optimal number for your model. Increase to 5â€“6 for broader questions. Only use âš reload values (7+) when you need wide document coverage and can wait for the model to reload its context window.

---

## ğŸ“ Version History

### Version 2.0 (Current)

**New features:**
- â˜ï¸ **External AI APIs** â€” six cloud providers now integrated: ChatGPT (OpenAI), Claude (Anthropic), Gemini (Google), Grok (xAI), Llama API (Meta), Mistral Large. API keys managed entirely in Settings with save, show/hide toggle, live ğŸ”Œ Test connection, and ğŸ”‘ Get Key buttons. Status dots show per-provider readiness at a glance.
- ğŸ”„ **Auto-fallback** â€” if a cloud provider fails or hits its rate limit, AI Prowler transparently falls back to local Ollama and notes the fallback in the answer.
- ğŸ¯ **AI Provider selector** â€” dropdown in the Ask Questions tab lets you pick the AI for each query. Live coloured status light reflects provider readiness.
- ğŸ“ **Attachments panel** â€” attach images and text files to questions. Images are sent to vision-capable cloud providers; text files are injected into the prompt.
- ğŸ“„ **File Output Mode** â€” when enabled, the AI labels code blocks with filenames. Detected files appear in a **ğŸ“ Files in Answer** panel with per-file ğŸ’¾ Save buttons â€” no copy-paste.
- ğŸ’¾ **Save Answer button** â€” save the full answer to `.txt` or `.md` with a single click.
- ğŸ” **Debug View** â€” checkbox in Settings â†’ Query Output controls whether Ollama/subprocess windows appear in the foreground or run silently in the background.
- ğŸ… **RAM-aware model selector** â€” model dropdown now shows âœ…/âš ï¸ fit badges based on detected system RAM, download size, and minimum RAM requirement. Models are sorted so compatible ones appear first.
- ğŸ”­ **Browse & Install Modelâ€¦** â€” opens a full model browser for downloading new Ollama models from within the app.
- âš ï¸ **Context chunks reload warnings** â€” values â‰¥7 are labelled "âš reload" to signal that the model will need to reload its context window. Changing the chunk count triggers an automatic re-prewarm at the required context size.
- â› **Enable debug output** checkbox â€” show/hide â± timing markers and ğŸ”¬ debug annotations in answers without restarting.

**Fixes:**
- `qo_frame` NameError in `create_settings_tab()` resolved â€” renamed to `output_frame` to match the enclosing LabelFrame.
- `debug_view` not persisting across restarts â€” `save_config()` in `rag_preprocessor.py` was missing the `debug_view` parameter, so the value was silently discarded. Parameter added to both the function signature and the save block.

---

### Version 1.9

**New features:**
- ğŸŸ¢ Auto-start Ollama â€” new "Ollama Server" section in Settings. When enabled, AI Prowler launches `ollama serve` automatically on startup and shuts it down on exit. Saves and restores across sessions.
- âš¡ Load AI Model button â€” manual pre-warm trigger in the Ask Questions tab. Start loading the model while you are still navigating other tabs.
- â¹ Stop query button â€” cancel a running query without closing the application.
- ğŸ”µ Model status indicator â€” coloured dot (grey/yellow/green) and text label in the Ask Questions tab shows real-time model load state.
- ğŸ›  UNINSTALL.bat PATH step rewritten â€” Step 3 now uses PowerShell exclusively, eliminating the hang caused by piping long PATH strings through CMD's `echo | find`.

**Fixes:**
- `ping_ollama` NameError on startup resolved â€” replaced with the correctly imported `check_ollama_available()` from `rag_preprocessor.py`.

---

### Version 1.8

**New features:**
- ğŸ¤ Voice input with Whisper large-v3-turbo, auto-stop silence detection, adjustable threshold
- â¸ Pause / Resume indexing â€” freeze mid-run and continue from exactly where you stopped
- ğŸ“¬ Per-email incremental indexing for `.mbox`, `.rmail`, `.babyl`, `.mmdf` â€” Message-ID deduplication, near-instant Stop, automatic cleanup of deleted messages
- ğŸ—‚ Auto Scan Config tab â€” live editor for supported/skipped extensions and skipped directories
- ğŸ“ Multi-folder queue with custom tree browser, Ctrl/Shift multi-select, mix of folders and individual files
- ğŸ” Pre-scan mode â€” preview what will be indexed before committing
- âš¡ GPU acceleration controls â€” Detect GPU, set layers, Apply & Reload without restarting
- ğŸ”„ Per-directory Remove with full vector and tracking cleanup
- ğŸ“Š Live elapsed timers on both the indexing and query progress bars
- ğŸ› Named tab index constants â€” adding/reordering tabs no longer silently breaks prewarm

**Fixes:**
- Stop button now responds after each individual email message (was: waited for entire archive file)
- INSTALL.bat fallback `requirements.txt` generator now includes all 11 packages including speech
- UNINSTALL.bat Whisper removal targets only the AI Prowler model, not the entire HuggingFace cache
- UNINSTALL.bat step counter corrected (was mixed 1/7â€“9/9, now consistent 1/9â€“9/9)
- Version numbers synchronised between GUI and engine (both 1.8)

---

## ğŸ‰ You're Ready!

You now know how to:

âœ… Install AI Prowler completely  
âœ… Index documents, code, and email from every major provider  
âœ… Export email from Gmail, Apple Mail, Thunderbird, Yahoo, and Outlook  
âœ… Ask questions with text, voice, or file attachments  
âœ… Use cloud AI providers for higher-quality answers  
âœ… Get one-click Save buttons for AI-written code files  
âœ… Pre-warm the AI model with the Load AI Model button  
âœ… Keep your index current with smart incremental updates  
âœ… Schedule automatic background updates  
âœ… Configure Ollama auto-start for a seamless one-click experience  
âœ… Customise scan behaviour for your workflow  
âœ… Tune GPU acceleration for your hardware  
âœ… Troubleshoot every common issue  

**Start exploring your documents with AI!** ğŸš€

---

*AI Prowler v2.0 â€” Your Personal AI Knowledge Base*  
*Local-first &nbsp;â€¢&nbsp; Cloud-optional &nbsp;â€¢&nbsp; 100% Yours*
